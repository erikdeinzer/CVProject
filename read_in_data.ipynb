{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb6YUgKM5H_i"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRq4TTVdkA7d",
        "outputId": "2ae95144-667d-4709-99f1-7423834c5656"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['.config', 'sample_data']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "os.listdir('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIH6Bi-QtyPd",
        "outputId": "0825756b-7702-4b3a-9ed7-b4aa222ec62d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/binh234/ccpd2019?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12.3G/12.3G [03:33<00:00, 61.6MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/binh234/ccpd2019/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "\n",
        "data_dir = 'data/'\n",
        "\n",
        "os.makedirs(data_dir)\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"binh234/ccpd2019\", path=data_dir)\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjMMzRXEwKqH",
        "outputId": "320685f7-4850-489f-a1c1-75cc3eaf510f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting CCPD: 355035it [27:26, 215.61it/s]\n"
          ]
        }
      ],
      "source": [
        "import tarfile\n",
        "\n",
        "# Define your target folder\n",
        "output_dir = path\n",
        "\n",
        "archive_path = f\"{path}/CCPD2019.tar.xz\"\n",
        "\n",
        "\n",
        "output_dir = \"data\"\n",
        "\n",
        "with tarfile.open(\"/root/.cache/kagglehub/datasets/binh234/ccpd2019/versions/1/CCPD2019.tar.xz\", mode=\"r:xz\") as tar:\n",
        "    for member in tqdm(tar, desc=\"Extracting CCPD\"):  # no total\n",
        "        tar.extract(member, path=output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2bPw-5W4hJe"
      },
      "source": [
        "# Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjsGOQfMuf5-",
        "outputId": "5f52956b-b018-407d-a32c-2be2d0d7bac9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['ccpd_challenge.txt',\n",
              " 'ccpd_tilt.txt',\n",
              " 'test.txt',\n",
              " 'ccpd_rotate.txt',\n",
              " 'ccpd_blur.txt',\n",
              " 'ccpd_db.txt',\n",
              " 'ccpd_fn.txt',\n",
              " 'train.txt',\n",
              " 'val.txt']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir('data/CCPD2019/splits')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOoNTUlaPFiD",
        "outputId": "4651fd92-6aad-49ba-a725-af8b3082fe96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ccpd_base/0092816091954-94_82-181&490_358&548-363&554_189&540_190&484_364&498-0_0_28_29_16_29_32-133-13.jpg\n",
            "ccpd_base/0104418103448-91_84-329&442_511&520-515&519_340&508_326&447_501&458-0_0_33_18_25_26_26-166-27.jpg\n",
            "ccpd_base/023275862069-90_86-173&473_468&557-485&563_189&555_187&469_483&477-0_0_2_27_9_26_24-178-36.jpg\n",
            "ccpd_base/0344827586207-92_75-255&369_564&505-560&520_256&454_239&349_543&415-0_0_11_32_27_27_33-37-19.jpg\n",
            "ccpd_base/0144516283524-97_72-90&538_280&616-278&629_95&595_85&525_268&559-0_0_19_30_28_32_19-74-41.jpg\n",
            "ccpd_base/00885536398467-90_89-301&521_492&580-501&578_300&589_297&523_498&512-0_0_24_32_32_29_26-98-33.jpg\n",
            "ccpd_base/0288697318007-88_89-195&525_496&636-508&623_198&644_193&535_503&514-0_0_10_4_27_32_31-156-45.jpg\n",
            "ccpd_base/048429118774-85_96-114&333_470&486-451&443_128&481_137&367_460&329-0_0_15_15_32_24_24-43-62.jpg\n",
            "ccpd_base/00865900383142-83_97-516&622_643&695-642&675_526&696_519&637_635&616-0_0_21_6_26_25_24-113-29.jpg\n",
            "ccpd_base/0274125957855-94_83-167&496_450&588-439&608_179&598_194&493_454&503-0_0_8_17_31_26_31-151-40.jpg\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with open('data/CCPD2019/splits/train.txt', 'r') as f:\n",
        "    test = [next(f) for _ in range(10)]\n",
        "    for line in test:\n",
        "        print(line.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iDp8l41Px9f",
        "outputId": "1a8de0dc-7582-42a0-baaf-849f8dfaa64a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/CCPD2019/ccpd_base/0092816091954-94_82-181&490_358&548-363&554_189&540_190&484_364&498-0_0_28_29_16_29_32-133-13.jpg: JPEG image data, baseline, precision 8, 720x1160, components 3\n",
            "data/CCPD2019/ccpd_base/0274125957855-94_83-167&496_450&588-439&608_179&598_194&493_454&503-0_0_8_17_31_26_31-151-40.jpg: JPEG image data, baseline, precision 8, 720x1160, components 3\n"
          ]
        }
      ],
      "source": [
        "!file \"data/CCPD2019/ccpd_base/0092816091954-94_82-181&490_358&548-363&554_189&540_190&484_364&498-0_0_28_29_16_29_32-133-13.jpg\"\n",
        "!file \"data/CCPD2019/ccpd_base/0274125957855-94_83-167&496_450&588-439&608_179&598_194&493_454&503-0_0_8_17_31_26_31-151-40.jpg\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sccp_ATPOnDM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEqYyNO1VZtl"
      },
      "outputs": [],
      "source": [
        "data_root = './data/CCPD2019'\n",
        "split = 'train'\n",
        "\n",
        "ann_file = os.path.join(data_root, 'splits', f'{split}.txt')\n",
        "\n",
        "# Read in filenames\n",
        "filenames = []\n",
        "with open(ann_file, 'r') as fn:\n",
        "    filenames = [line.strip() for line in fn if line.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTVSusWTXdxp"
      },
      "outputs": [],
      "source": [
        "# The annotations are savedin the filename\n",
        "\n",
        "class CCPVAnnotation():\n",
        "  def __init__(self, filename):\n",
        "    self.filename = filename\n",
        "    self.img_size = (720, 1160)\n",
        "    label = filename.split('/')[-1][:-4] # Get Filename minus the ending\n",
        "\n",
        "    self.id, tilt, bb, polygonial, lpn, self.brightness, self.blurriness  = label.split('-')\n",
        "\n",
        "    self.tilt = tilt.split(\"_\")\n",
        "\n",
        "    # Coarse Bounding Box\n",
        "    tl, br = bb.split('_')\n",
        "    x1, y1 = map(int, tl.split('&'))\n",
        "    x2, y2 = map(int, br.split('&'))\n",
        "    self.xyxy = ((x1,y1),(x2,y2))\n",
        "\n",
        "    # Make yolo output\n",
        "    normed_center = ((x1+x2)/(2*1160), (y1+y2)/(2*720))\n",
        "    width, height = (x2-x1), (y2-y1)\n",
        "\n",
        "    self.yolo = {\n",
        "        'normed_center': normed_center,\n",
        "        'bbox_width': width / self.img_size[1],\n",
        "        'bbox_height': height / self.img_size[0]\n",
        "    }\n",
        "\n",
        "    self.coco = {\n",
        "        'x_min': x1,\n",
        "        'y_min': y1,\n",
        "        'bbox_width': width,\n",
        "        'bbox_height': height\n",
        "    }\n",
        "\n",
        "    # Polygon\n",
        "    br,tr,tl,bl = polygonial.split('_')\n",
        "    self.polygon = [tuple(map(int, vert.split('&'))) for vert in [br, tr, tl, bl]]\n",
        "\n",
        "    # License Plate number\n",
        "    self.lpn_coded = lpn.split('_')\n",
        "\n",
        "\n",
        "\n",
        "  def decode_lpn(self, lpn = None):\n",
        "    \"\"\"\n",
        "    Decode license plate number\n",
        "\n",
        "    maybe better to use in the model itself as output.\n",
        "    \"\"\"\n",
        "    provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
        "    alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
        "                'X', 'Y', 'Z', 'O']\n",
        "    ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
        "          'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
        "\n",
        "    if lpn is None: lpn = self.lpn_coded\n",
        "\n",
        "    digits = lpn\n",
        "\n",
        "    province = provinces[int(digits[0])]\n",
        "    first = alphabets[int(digits[1])]\n",
        "    chars = [ads[int(i)] for i in digits[2:]]\n",
        "\n",
        "    decoded_lp = [province] + [first] + chars\n",
        "    return decoded_lp\n",
        "\n",
        "  def __repr__(self) -> str:\n",
        "      return str({\n",
        "        'id': self.id,\n",
        "        'tilt': self.tilt,\n",
        "        'xyxy': self.xyxy,\n",
        "        'polygon': self.polygon,\n",
        "        'coded lpn': self.lpn_coded,\n",
        "        'decoded_lpn': self.decode_lpn(),\n",
        "        'brightness': self.brightness,\n",
        "        'blurriness': self.blurriness\n",
        "    })\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0kkfAOKPO5z"
      },
      "outputs": [],
      "source": [
        "ann = CCPVAnnotation('data/CCPD2019/ccpd_base/0092816091954-94_82-181&490_358&548-363&554_189&540_190&484_364&498-0_0_28_29_16_29_32-133-13.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fGYvtikPjA5",
        "outputId": "bd260b2a-d582-4548-8ae6-eb9dbd3a2827"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['皖', 'A', '4', '5', 'S', '5', '8']"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ann.decode_lpn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE5mR5OrXnKh",
        "outputId": "70c96775-9381-4862-a9e1-a5f33af0c640"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '0092816091954', 'tilt': ['94', '82'], 'xyxy': ((181, 490), (358, 548)), 'polygon': [(363, 554), (189, 540), (190, 484), (364, 498)], 'coded lpn': ['0', '0', '28', '29', '16', '29', '32'], 'decoded_lpn': ['皖', 'A', '4', '5', 'S', '5', '8'], 'brightness': '133', 'blurriness': '13'}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ann"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmtbh0e_P8rv"
      },
      "source": [
        "# Data Informations\n",
        "\n",
        "Input pictures: 720x1160\n",
        "\n",
        "Data Information encapsulated in filename:\n",
        "\n",
        "{ID}-{blur_angle}-{t&l_b&r}-{polygon (4pnts)}-{label_indices}-{brightness}-{tilt}.jpg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2TAq6B5UBI"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9D1hwK2SYHg"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Tuple, List\n",
        "\n",
        "@dataclass\n",
        "class CCPDSample:\n",
        "    image_path: str\n",
        "    label: str                          # Decoded license plate string\n",
        "    aabb: Tuple[int, int, int, int]     # (x_min, y_min, x_max, y_max)\n",
        "    polygon: List[Tuple[int, int]]      # List of 4 corner points (x, y)\n",
        "    blur: int = None\n",
        "    angle: int = None\n",
        "    brightness: int = None\n",
        "    tilt: int = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT1Q2y0kuuRT"
      },
      "outputs": [],
      "source": [
        "class Registry():\n",
        "    def __init__(self, name='default'):\n",
        "        self.name = name\n",
        "        self._registry = {}\n",
        "\n",
        "    def register(self, obj=None, *, type=None):\n",
        "        if obj is None:\n",
        "            return lambda obj: self.register(obj, type=type)\n",
        "        key = type or obj.__name__\n",
        "        if key in self._registry:\n",
        "            raise ValueError('Duplicate registrations with same name not possible')\n",
        "        self._registry[key]=obj\n",
        "        return obj\n",
        "\n",
        "    def get(self, type):\n",
        "        if type not in self._registry:\n",
        "            raise KeyError(f\"'{type}' not found in the '{self.name}' registry.\")\n",
        "        obj = self._registry.get(type)\n",
        "        return obj\n",
        "\n",
        "    def all(self):\n",
        "        return self._registry\n",
        "\n",
        "    def __contains__(self, type):\n",
        "        return type in self._registry\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._registry)\n",
        "\n",
        "MODELS = Registry('MODELS')\n",
        "DATASETS = Registry('DATASETS')\n",
        "EVALUATIONS = Registry('EVALUATIONS')\n",
        "TRANSFORMS = Registry('TRANSFORMS')\n",
        "\n",
        "class Builder():\n",
        "    def __init__(self, registry, **kwargs):\n",
        "        self._registry = registry\n",
        "        self.config = kwargs\n",
        "\n",
        "    def build_module(self, type, config):\n",
        "        if type not in self._registry:\n",
        "            raise KeyError(f\"'{type}' not registered in registry.\")\n",
        "        try:\n",
        "            return self._registry.get(type)(**config)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error instantiating '{type}': {e}\") from e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "S0gAALVxT7kM",
        "outputId": "aad8ad03-a10e-48c7-fd30-acb0f8b8feb0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.transforms.autoaugment.RandAugment</b><br/>def _wrapped_call_impl(*args, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/torchvision/transforms/autoaugment.py</a>RandAugment data augmentation method based on\n",
              "`&quot;RandAugment: Practical automated data augmentation with a reduced search space&quot;\n",
              "&lt;https://arxiv.org/abs/1909.13719&gt;`_.\n",
              "If the image is torch Tensor, it should be of type torch.uint8, and it is expected\n",
              "to have [..., 1 or 3, H, W] shape, where ... means an arbitrary number of leading dimensions.\n",
              "If img is PIL Image, it is expected to be in mode &quot;L&quot; or &quot;RGB&quot;.\n",
              "\n",
              "Args:\n",
              "    num_ops (int): Number of augmentation transformations to apply sequentially.\n",
              "    magnitude (int): Magnitude for all the transformations.\n",
              "    num_magnitude_bins (int): The number of different magnitude values.\n",
              "    interpolation (InterpolationMode): Desired interpolation enum defined by\n",
              "        :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.NEAREST``.\n",
              "        If input is Tensor, only ``InterpolationMode.NEAREST``, ``InterpolationMode.BILINEAR`` are supported.\n",
              "    fill (sequence or number, optional): Pixel fill value for the area outside the transformed\n",
              "        image. If given a number, the value is used for all bands respectively.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 287);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ],
            "text/plain": [
              "torchvision.transforms.autoaugment.RandAugment"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "transform = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(),\n",
        "    A.Rotate(limit=10),\n",
        "    A.Normalize(),\n",
        "    ToTensorV2()\n",
        "],\n",
        "    bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']),\n",
        "    keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)\n",
        ")\n",
        "\n",
        "\n",
        "TRANSFORMS.register(A.ToTensorV2, type='ToTensor')\n",
        "TRANSFORMS.register(A.Normalize, type='Normalize')\n",
        "TRANSFORMS.register(A.Resize, type='Resize')\n",
        "TRANSFORMS.register(A.ColorJitter, type='ColorJitter')\n",
        "TRANSFORMS.register(A.RandomHorizontalFlip, type='RandomHorizontalFlip')\n",
        "TRANSFORMS.register(A.RandomRotation, type='RandomRotation')\n",
        "TRANSFORMS.register(A.RandomAffine, type='RandomAffine')\n",
        "TRANSFORMS.register(A.RandomCrop, type='RandomCrop')\n",
        "TRANSFORMS.register(A.RandomErasing, type='RandomErasing')\n",
        "TRANSFORMS.register(A.RandomPerspective, type='RandomPerspective')\n",
        "TRANSFORMS.register(A.RandomResizedCrop, type='RandomResizedCrop')\n",
        "TRANSFORMS.register(A.RandomVerticalFlip, type='RandomVerticalFlip')\n",
        "TRANSFORMS.register(A.RandomAdjustSharpness, type='RandomAdjustSharpness')\n",
        "TRANSFORMS.register(A.RandomAutocontrast, type='RandomAutocontrast')\n",
        "TRANSFORMS.register(A.RandomEqualize, type='RandomEqualize')\n",
        "TRANSFORMS.register(A.RandomInvert, type='RandomInvert')\n",
        "TRANSFORMS.register(A.RandomSolarize, type='RandomSolarize')\n",
        "TRANSFORMS.register(A.AugMix, type='AugMix')\n",
        "TRANSFORMS.register(A.AutoAugment, type='AutoAugment')\n",
        "TRANSFORMS.register(A.RandAugment, type='RandAugment')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoQYEe4V5j5E"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYypMNabutxW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class CCPD(Dataset):\n",
        "    def __init__(self, data_root, split='train', pipeline=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_root (str): Root directory containing CCPD image data.\n",
        "            split (str): Dataset split - typically 'train', 'val', or 'test'.\n",
        "            transforms (list[dict]): Transform pipeline defined in config.\n",
        "        \"\"\"\n",
        "        self.data_root = os.path.join(data_root, split)\n",
        "\n",
        "        self.ann_file = os.path.join(data_root, 'splits', f'{split}.txt')\n",
        "        self.image_files = [f for f in os.listdir(self.ann_file) if f.endswith('.jpg') or f.endswith('.png')]\n",
        "\n",
        "\n",
        "        self.transforms = self._build_transforms(pipeline)\n",
        "\n",
        "    def _build_transforms(self, transform_cfgs):\n",
        "        transforms_builder = Builder(TRANSFORMS)\n",
        "        if transform_cfgs is None:\n",
        "            return None\n",
        "        transform_list = []\n",
        "        for cfg in transform_cfgs:\n",
        "            transform = transforms_builder.build_module(**cfg)\n",
        "            transform_list.append(transform)\n",
        "        return T.Compose(transform_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def _parse_label_from_filename(self, filename):\n",
        "        # Example filename: 080811_136_275&404_249&434-94&164_263&164_263&191_94&191-0_0_7_5_2_3_4-28-26.jpg\n",
        "        # Last part before extension is plate characters\n",
        "        try:\n",
        "            label_str = filename.split('-')[-3]\n",
        "            label = [int(char) for char in label_str.split('_')]  # Convert to int if using digits as class IDs\n",
        "            return label\n",
        "        except Exception as e:\n",
        "            raise ValueError(f\"Could not parse label from filename: {filename}\") from e\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.image_files[idx]\n",
        "        img_path = os.path.join(self.data_root, filename)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self._parse_label_from_filename(filename)\n",
        "\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return {'img': image, 'label': torch.tensor(label, dtype=torch.long)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhLCflR28fxo"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vF1JP4dutN_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjiqGK4q42zP"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv-GRhwZutAf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A05HEglkxQPy"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUZt02eiusq0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
